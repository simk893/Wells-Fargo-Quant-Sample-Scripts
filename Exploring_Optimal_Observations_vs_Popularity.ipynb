{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas\n",
    "import numpy\n",
    "import seaborn\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_distribution(data_dict):\n",
    "    start_dates = [datetime.strptime(info['observation_start'], '%Y-%m-%d') for info in data_dict.values()]\n",
    "    popularity_scores = [info['popularity'] for info in data_dict.values()]\n",
    "    \n",
    "    start_dates_df = pandas.DataFrame({'start_date': start_dates, 'popularity': popularity_scores})\n",
    "    full_date_range = pandas.date_range(start_dates_df['start_date'].min(), start_dates_df['start_date'].max())\n",
    "    full_date_df = pandas.DataFrame(full_date_range, columns=['date'])\n",
    "    \n",
    "    total_series = len(start_dates_df)\n",
    "    full_date_df['cumulative_count'] = full_date_df['date'].apply(lambda x: (start_dates_df['start_date'] <= x).sum())\n",
    "    full_date_df['cumulative_percentage'] = (full_date_df['cumulative_count'] / total_series) * 100\n",
    "    full_date_df['average_popularity'] = full_date_df['date'].apply(lambda x: start_dates_df[start_dates_df['start_date'] <= x]['popularity'].mean())\n",
    "    \n",
    "    max_average_popularity = full_date_df['average_popularity'].max()\n",
    "    full_date_df['average_popularity_percentage'] = (full_date_df['average_popularity'] / max_average_popularity) * 100\n",
    "    \n",
    "    matplotlib.pyplot.figure(figsize=(14, 7))\n",
    "    matplotlib.pyplot.plot(full_date_df['date'], full_date_df['cumulative_percentage'], label='Cumulative Percentage')\n",
    "    matplotlib.pyplot.plot(full_date_df['date'], full_date_df['average_popularity_percentage'], label='Average Popularity (%)', color='orange')\n",
    "    matplotlib.pyplot.xlabel('Date')\n",
    "    matplotlib.pyplot.ylabel('Cumulative Percentage / Average Popularity (%)')\n",
    "    matplotlib.pyplot.title('Cumulative Distribution and Average Popularity')\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.grid(True)\n",
    "    matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://fred.stlouisfed.org/categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get FRED Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_file(json_file, start_criteria='2000-01-01', end_criteria='2024-01-01'):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data_dict = json.load(file)\n",
    "    \n",
    "    # Filter the series based on the criteria\n",
    "    filtered_series = {k: v for k, v in data_dict.items() if (v['observation_start'] <= start_criteria) and (v['observation_end'] >= end_criteria)}\n",
    "    \n",
    "    # Create the FRED dataframe\n",
    "    DF = create_FRED_dataframe(filtered_series, start_date=start_criteria)\n",
    "    \n",
    "    if DF is not None:\n",
    "        # Save the dataframe to a CSV file\n",
    "        output_csv_path = os.path.splitext(json_file)[0] + '_Dataframe.csv'\n",
    "        DF.to_csv(output_csv_path, index=True)\n",
    "        print(f\"Processed {json_file} and saved to {output_csv_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to create dataframe for {json_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= r\"C:\\Users\\User\\OneDrive\\Desktop\\UNC\\Fall 2024\\BUSI 880\\Project\\Daily_Series.json\"\n",
    "process_json_file(file_path, start_criteria='2000-01-01', end_criteria='2023-12-29')file_path= r\"C:\\Users\\User\\OneDrive\\Desktop\\UNC\\Fall 2024\\BUSI 880\\Project\\Quarterly_Series.json\"\n",
    "process_json_file(file_path)\n",
    "file_path= r\"C:\\Users\\User\\OneDrive\\Desktop\\UNC\\Fall 2024\\BUSI 880\\Project\\Monthly_Series.json\"\n",
    "process_json_file(file_path)\n",
    "file_path= r\"C:\\Users\\User\\OneDrive\\Desktop\\UNC\\Fall 2024\\BUSI 880\\Project\\Annual_Series.json\"\n",
    "process_json_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data before applying PCA\n",
    "FRED_Data = create_FRED_dataframe(Daily_Data).pct_change()\n",
    "FRED_Data.replace([numpy.inf, -numpy.inf], numpy.nan, inplace=True)\n",
    "FRED_Data_scaled = StandardScaler().fit_transform(FRED_Data.dropna())\n",
    "\n",
    "# Plot the correlation matrix\n",
    "correlation_matrix = pandas.DataFrame(FRED_Data_scaled, columns=FRED_Data.columns).corr()\n",
    "seaborn.heatmap(correlation_matrix, annot=False, cmap='coolwarm', xticklabels=True, yticklabels=True)\n",
    "matplotlib.pyplot.xticks(fontsize=5)\n",
    "matplotlib.pyplot.yticks(fontsize=5)\n",
    "matplotlib.pyplot.title('Correlation Matrix')\n",
    "matplotlib.pyplot.show()\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=10)\n",
    "principal_components = pca.fit_transform(FRED_Data_scaled)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "principal_df = pandas.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
    "\n",
    "# Plot the histogram of the variance explained by each principal component\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "matplotlib.pyplot.bar(range(1, len(explained_variance) + 1), explained_variance)\n",
    "matplotlib.pyplot.xlabel('Principal Component')\n",
    "matplotlib.pyplot.ylabel('Variance Explained (%)')\n",
    "matplotlib.pyplot.title('Variance Explained by Principal Components')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of principal components to plot\n",
    "K = 3\n",
    "components = pca.components_[:K]\n",
    "components_df = pandas.DataFrame(components, columns=FRED_Data.columns, index=[f'PC{i+1}' for i in range(K)])\n",
    "\n",
    "# Plot the principal components as a bar graph\n",
    "components_df.T.plot(kind='bar', figsize=(14, 8))\n",
    "matplotlib.pyplot.xlabel('Variables')\n",
    "matplotlib.pyplot.ylabel('Principal Component Loading')\n",
    "matplotlib.pyplot.title(f'Principal Component Loadings for First {K} PCs')\n",
    "matplotlib.pyplot.legend(title='Principal Components')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_histograms(df, K):\n",
    "\n",
    "    # Standardize the data before applying PCA\n",
    "    df.replace([numpy.inf, -numpy.inf], numpy.nan, inplace=True)\n",
    "    df_scaled = StandardScaler().fit_transform(df.dropna())\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=K)\n",
    "    principal_components = pca.fit_transform(df_scaled)\n",
    "\n",
    "    # Create a DataFrame with the principal components\n",
    "    principal_df = pandas.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(K)])\n",
    "\n",
    "    # Plot histograms for the first K principal components\n",
    "    fig, axes = matplotlib.pyplot.subplots(nrows=1, ncols=K, figsize=(14, 6))\n",
    "    for i in range(K):\n",
    "        pc = principal_df[f'PC{i+1}']\n",
    "        axes[i].hist(pc, bins=1000, edgecolor='black')\n",
    "        axes[i].set_xlabel(f'PC{i+1} Values')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].set_title(f'Distribution of PC{i+1} Values')\n",
    "\n",
    "    matplotlib.pyplot.tight_layout()\n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_pca_histograms(FRED_Data, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(principal_df, K):\n",
    "\n",
    "    # Extract the principal components\n",
    "    pcs = [principal_df[f'PC{i+1}'] for i in range(K)]\n",
    "    means = [pc.mean() for pc in pcs]\n",
    "    stds = [pc.std() for pc in pcs]\n",
    "\n",
    "    # Define the standard deviation ranges\n",
    "    std_ranges = [-2, -1, 1, 2]\n",
    "\n",
    "    # Function to categorize the values into standard deviation ranges\n",
    "    def categorize_std(value, mean, std):\n",
    "        if value < mean - std:\n",
    "            return -2\n",
    "        elif value <= mean and value >= mean - std:\n",
    "            return -1\n",
    "        elif value > mean and value <= mean + std:\n",
    "            return 1\n",
    "        elif value > mean + std:\n",
    "            return 2\n",
    "\n",
    "    # Categorize the principal component values\n",
    "    categories = [pc.apply(categorize_std, args=(mean, std)) for pc, mean, std in zip(pcs, means, stds)]\n",
    "\n",
    "    # Initialize the transition matrix\n",
    "    num_states = len(std_ranges) ** K\n",
    "    transition_matrix = numpy.zeros((num_states, num_states))\n",
    "\n",
    "    # Count the transitions\n",
    "    for i in range(1, len(categories[0])):\n",
    "        from_state = tuple(std_ranges.index(categories[j].iloc[i-1]) for j in range(K))\n",
    "        to_state = tuple(std_ranges.index(categories[j].iloc[i]) for j in range(K))\n",
    "        from_index = sum(from_state[j] * (len(std_ranges) ** (K - j - 1)) for j in range(K))\n",
    "        to_index = sum(to_state[j] * (len(std_ranges) ** (K - j - 1)) for j in range(K))\n",
    "        transition_matrix[from_index, to_index] += 1\n",
    "\n",
    "    # Normalize to get probabilities\n",
    "    transition_matrix = transition_matrix / transition_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Convert to DataFrame for better readability\n",
    "    index_labels = [f'{std_ranges[(i // (len(std_ranges) ** (K - j - 1))) % len(std_ranges)]}' for i in range(num_states) for j in range(K)]\n",
    "    index_labels = [f'{\" \".join([f\"+{label}\" if float(label) > 0 else label for label in index_labels[i:i+K]])}' for i in range(0, len(index_labels), K)]\n",
    "    transition_df = pandas.DataFrame(transition_matrix, index=index_labels, columns=index_labels)\n",
    "    transition_df.index.name = 'From'\n",
    "    transition_df.columns.name = 'To'\n",
    "\n",
    "    # Plot the transition matrix as a heatmap\n",
    "    matplotlib.pyplot.figure(figsize=(12, 10))\n",
    "    seaborn.heatmap(transition_df, annot=False, fmt='0.2f', cmap='Greys', cbar=True, xticklabels=True, yticklabels=True, vmin=0, vmax=1)\n",
    "    matplotlib.pyplot.title('Markov Transition Matrix')\n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "    return transition_df\n",
    "\n",
    "# Example usage:\n",
    "transition_df = create_transition_matrix(principal_df, K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
